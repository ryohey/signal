from pydantic import BaseModel, Field
from typing import Optional
from enum import Enum


class GenerateRequest(BaseModel):
    prompt: str
    agent_type: Optional[str] = "composition_agent"  # "llm" or "composition_agent"


# ============================================================================
# SongSpec Models (for deep agent architecture)
# ============================================================================


class SectionType(str, Enum):
    INTRO = "intro"
    VERSE = "verse"
    PRE_CHORUS = "pre_chorus"
    CHORUS = "chorus"
    BRIDGE = "bridge"
    OUTRO = "outro"
    SOLO = "solo"


class InstrumentSpec(BaseModel):
    name: str  # e.g., "drums", "bass", "lead_guitar"
    role: str  # e.g., "rhythm", "melody", "harmony", "texture"
    channel: int = Field(ge=0, le=15)
    program: int = Field(ge=0, le=127)
    note_range: tuple[int, int] = Field(default=(36, 84))  # MIDI note range
    style_notes: str  # e.g., "syncopated eighth notes, lock with kick"


class SectionSpec(BaseModel):
    type: SectionType
    bars: int = Field(ge=1, le=32)
    energy: str  # "low", "medium", "high", "building", "fading"
    active_instruments: list[str]  # instrument names active in this section
    notes: Optional[str] = None  # additional section-specific guidance


class ChordProgression(BaseModel):
    section_type: SectionType
    chords: list[str]  # e.g., ["Am", "F", "C", "G"]
    bars_per_chord: int = 2


class SongSpec(BaseModel):
    """Structured specification for a song, generated by planning stage."""

    tempo: int = Field(ge=40, le=240)
    key: str
    time_signature: str = "4/4"
    total_bars: int
    style: str  # genre/mood description

    instruments: list[InstrumentSpec]
    structure: list[SectionSpec]
    chord_progressions: list[ChordProgression]

    # Quality targets (used for validation)
    target_note_density: str = "medium"  # "sparse", "medium", "dense"
    target_rhythm_complexity: str = "medium"  # "simple", "medium", "complex"

    # Additional guidance
    reference_artists: Optional[list[str]] = None
    style_notes: Optional[str] = None


class TrackData(BaseModel):
    name: str
    midi_data: str  # base64 encoded
    channel: int
    program_number: int


class SongMetadata(BaseModel):
    tempo: int
    key: str
    time_signature: str


class GenerateResponse(BaseModel):
    tracks: list[TrackData]
    metadata: SongMetadata
    message: str


class RegenerateRequest(BaseModel):
    track_name: str
    instruction: str
    context: dict  # Current song state for context


class RegenerateResponse(BaseModel):
    track: TrackData
    message: str


class ErrorResponse(BaseModel):
    error: str
    detail: Optional[str] = None


# ============================================================================
# Validation Models (for deep agent architecture)
# ============================================================================


class TrackMetrics(BaseModel):
    """Metrics for a single track's MIDI data."""

    name: str
    note_count: int

    # Rhythm metrics
    eighth_note_or_faster_pct: float  # percentage of notes that are eighth or faster
    syncopation_score: float  # 0-1, how many notes are off-beat

    # Velocity metrics
    velocity_min: int
    velocity_max: int
    velocity_mean: float
    velocity_std: float

    # Density metrics
    notes_per_bar: float
    silence_pct: float  # percentage of time with no notes

    # Drum-specific metrics (only populated for drum tracks)
    snare_hits_per_bar: Optional[float] = None
    kick_hits_per_bar: Optional[float] = None
    hihat_hits_per_bar: Optional[float] = None


class ValidationResult(BaseModel):
    """Result of MIDI quality validation."""

    passed: bool
    overall_score: float  # 0-1

    track_metrics: list[TrackMetrics]

    # Specific issues found
    issues: list[str]  # e.g., ["drums: too sparse", "bass: no syncopation"]

    # LLM review summary
    llm_review: Optional[str] = None

    # Suggestions for improvement
    suggestions: list[str]


class AttemptLog(BaseModel):
    """Log entry for a single generation attempt."""

    attempt: int
    mode: str  # "initial", "patch", "regenerate"
    code_generated: bool
    execution_success: bool
    validation_passed: bool
    error: Optional[str] = None
    validation_result: Optional[ValidationResult] = None
    issues: Optional[list[str]] = None


# ============================================================================
# Hybrid Agent Models (frontend tool execution)
# ============================================================================


class ToolCall(BaseModel):
    """A tool call to be executed on the frontend."""

    id: str
    name: str
    args: dict


class ToolResult(BaseModel):
    """Result of a tool execution from the frontend."""

    id: str
    result: str  # JSON string result


class AgentStepRequest(BaseModel):
    """Request for an agent step."""

    prompt: Optional[str] = None  # For starting a new conversation
    thread_id: Optional[str] = None  # For continuing an existing conversation
    tool_results: Optional[list[ToolResult]] = None  # For resuming after tool execution
    context: Optional[str] = None  # Current song state for agent awareness


class AgentStepResponse(BaseModel):
    """Response from an agent step."""

    thread_id: str
    tool_calls: list[ToolCall]
    done: bool
    message: Optional[str] = None


# ============================================================================
# Pitch Detection Models (for voice-to-MIDI feature)
# ============================================================================


class PitchSegment(BaseModel):
    """A segment of detected pitch from audio."""

    start_time: float  # seconds
    end_time: float  # seconds
    avg_frequency: float  # Hz
    avg_confidence: float  # 0-1
    midi_note: int  # MIDI note number (0-127)


class PitchDetectionResponse(BaseModel):
    """Response from pitch detection endpoint."""

    segments: list[PitchSegment]
    duration_seconds: float
    sample_rate: int
    model_used: str  # 'tiny', 'small', 'medium', 'large', 'full'


class RhythmInterpretationRequest(BaseModel):
    """Request for rhythm interpretation."""

    segments: list[PitchSegment]
    quantize_value: int  # 4=quarter, 8=eighth, 16=sixteenth
    timebase: int  # ticks per quarter note (usually 480)
    project_tempo: float  # current project BPM as fallback
    # New fields for key quantization
    key_hint: Optional[int] = None  # 0-11 (C=0), if user has key set
    scale_hint: Optional[str] = None  # "major" or "minor", if user has key set


class InterpretedNote(BaseModel):
    """A note with interpreted rhythm."""

    note_number: int  # MIDI note 0-127
    tick: int  # start position in ticks
    duration: int  # duration in ticks
    velocity: int  # 1-127


class RhythmInterpretationResponse(BaseModel):
    """Response from rhythm interpretation endpoint."""

    notes: list[InterpretedNote]
    detected_tempo: float  # BPM
    tempo_confidence: str  # 'high', 'medium', 'low'
    time_signature: tuple[int, int]  # e.g., (4, 4)
    # New fields for key detection results
    detected_key: int = 0  # 0-11 (C=0)
    detected_scale: str = "major"  # "major" or "minor"
    key_confidence: str = "low"  # 'high', 'medium', 'low'
    pitch_offset_cents: float = 0.0  # how flat/sharp the singer was
    key_source: str = "detected_low_confidence"  # 'user_provided', 'detected_confident', 'detected_low_confidence'
